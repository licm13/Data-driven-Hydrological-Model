# æ°´æ–‡æ¨¡å‹ä»£ç åº“é‡æ„æ¶æ„æ–¹æ¡ˆ

## ğŸ“‹ Executive Summary

æœ¬é‡æ„æ–¹æ¡ˆæ—¨åœ¨å°† **Legacy Experimental Layer** çš„ä¸šåŠ¡é€»è¾‘ï¼ˆ4ä¸ªæ ¸å¿ƒå®éªŒï¼‰è¿ç§»å¹¶é›†æˆåˆ° **dMG Framework Layer** çš„ç°ä»£æ¶æ„ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„ã€æ”¯æŒå¾®åˆ†å‚æ•°å­¦ä¹ ï¼ˆdPLï¼‰çš„é«˜å¤æ‚åº¦æ°´æ–‡å»ºæ¨¡ç³»ç»Ÿã€‚

## ğŸ¯ é‡æ„ç›®æ ‡

1. **æ¶æ„ç»Ÿä¸€**ï¼šåŸºäº Hydra çš„é…ç½®é©±åŠ¨å®éªŒç³»ç»Ÿ
2. **æ¨¡å‹é€‚é…**ï¼šLegacy NumPy æ¨¡å‹ â†’ PyTorch Differentiable æ¨¡å‹
3. **æ•°æ®å±‚å‡çº§**ï¼šæ”¯æŒ CAMELS + CSV/ASCII + é‡‡æ ·ç­–ç•¥
4. **è®­ç»ƒèåˆ**ï¼šSpotpy ä¼ ç»Ÿæ ¡å‡† + PyTorch æ¢¯åº¦ä¸‹é™
5. **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡ YAML å®šä¹‰å®Œæ•´å®éªŒæµç¨‹

---

## ğŸ“ ç³»ç»Ÿæ¶æ„

### é«˜å±‚æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Unified Hydra Entry Point                       â”‚
â”‚                   (HBA-Model/src/dmg/__main__.py)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”œâ”€â”€ Config Manager (Hydra + OmegaConf)
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Experiment    â”‚   â”‚ Model Pipeline â”‚   â”‚ Data Pipelineâ”‚
â”‚ Task System   â”‚   â”‚                â”‚   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Learning Curve Experiment                â”‚
â”‚              Sampling Strategy Experiment             â”‚
â”‚              Information Entropy Experiment           â”‚
â”‚              Spatial Distribution Experiment          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶å±‚æ¬¡ç»“æ„

```
dMG Framework
â”œâ”€â”€ Core Infrastructure
â”‚   â”œâ”€â”€ dmg/core/data/loaders/
â”‚   â”‚   â”œâ”€â”€ hydro_loader.py (æ‰©å±•)
â”‚   â”‚   â””â”€â”€ legacy_csv_loader.py (æ–°)
â”‚   â”œâ”€â”€ dmg/core/data/samplers/
â”‚   â”‚   â”œâ”€â”€ base_sampler.py
â”‚   â”‚   â””â”€â”€ learning_curve_sampler.py (æ–°)
â”‚   â”œâ”€â”€ dmg/core/calc/
â”‚   â”‚   â”œâ”€â”€ metrics.py (æ‰©å±•)
â”‚   â”‚   â””â”€â”€ entropy.py (æ–°)
â”‚   â””â”€â”€ dmg/core/calibration/
â”‚       â”œâ”€â”€ base_calibrator.py (æ–°)
â”‚       â””â”€â”€ spotpy_calibrator.py (æ–°)
â”‚
â”œâ”€â”€ Models
â”‚   â”œâ”€â”€ dmg/models/phy_models/
â”‚   â”‚   â”œâ”€â”€ hbv_torch.py (æ–° - PyTorch å®ç°)
â”‚   â”‚   â”œâ”€â”€ gr4j_torch.py (æ–°)
â”‚   â”‚   â””â”€â”€ legacy_model_adapter.py (æ–° - Wrapper)
â”‚   â”œâ”€â”€ dmg/models/delta_models/
â”‚   â”‚   â””â”€â”€ dpl_model.py (ä¿æŒ)
â”‚   â””â”€â”€ dmg/models/neural_networks/
â”‚       â””â”€â”€ lstm.py (ä¿æŒ)
â”‚
â”œâ”€â”€ Training
â”‚   â”œâ”€â”€ dmg/trainers/
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ trainer.py (æ‰©å±•)
â”‚   â”‚   â””â”€â”€ hybrid_trainer.py (æ–° - Spotpy + Gradient)
â”‚   â””â”€â”€ dmg/trainers/strategies/
â”‚       â”œâ”€â”€ gradient_descent_strategy.py (æ–°)
â”‚       â””â”€â”€ spotpy_strategy.py (æ–°)
â”‚
â””â”€â”€ Experiments
    â”œâ”€â”€ dmg/experiments/ (æ–°æ¨¡å—)
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ base_experiment.py (æŠ½è±¡åŸºç±»)
    â”‚   â”œâ”€â”€ learning_curves.py (å®éªŒ1)
    â”‚   â”œâ”€â”€ sampling_strategies.py (å®éªŒ2)
    â”‚   â”œâ”€â”€ information_content.py (å®éªŒ3)
    â”‚   â””â”€â”€ spatial_distribution.py (å®éªŒ4)
    â””â”€â”€ dmg/experiments/tasks/
        â”œâ”€â”€ task_registry.py
        â””â”€â”€ task_executor.py
```

---

## ğŸ”§ å…³é”®ç»„ä»¶è®¾è®¡

### 1. PyTorch Model Adapter

**è®¾è®¡æ¨¡å¼**: Adapter Pattern + Template Method

```python
# HBA-Model/src/dmg/models/phy_models/legacy_model_adapter.py

class LegacyModelAdapter(torch.nn.Module):
    """
    å°† Legacy NumPy æ¨¡å‹é€‚é…ä¸º PyTorch Differentiable æ¨¡å‹

    ç­–ç•¥ï¼š
    1. Eager Mode: ç›´æ¥è½¬æ¢ NumPy â†’ Torch (æ€§èƒ½)
    2. Trace Mode: ä½¿ç”¨ torch.jit.trace (å…¼å®¹æ€§)
    3. Rewrite Mode: å®Œå…¨é‡å†™ä¸º PyTorch æ“ä½œ (æœ€ä¼˜)
    """

    def __init__(self, legacy_model, adaptation_strategy='eager'):
        super().__init__()
        self.legacy_model = legacy_model
        self.strategy = adaptation_strategy

    def forward(self, forcings, parameters):
        # è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
        if self.strategy == 'rewrite':
            return self._forward_torch(forcings, parameters)
        else:
            return self._forward_numpy_wrapped(forcings, parameters)
```

### 2. Unified Data Loader

**è®¾è®¡æ¨¡å¼**: Strategy Pattern + Factory Pattern

```python
# HBA-Model/src/dmg/core/data/loaders/universal_hydro_loader.py

class UniversalHydroLoader(BaseLoader):
    """
    ç»Ÿä¸€æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒï¼š
    - CAMELS dataset (NetCDF)
    - CSV/ASCII files (Legacy)
    - IMPRO format (ç‰¹æ®Šæ ¼å¼)
    - é‡‡æ ·ç­–ç•¥ï¼ˆDouglas-Peucker, Random, Stratifiedï¼‰
    """

    def __init__(self, config):
        super().__init__(config)
        self.format = config['observations']['format']
        self.loader_strategy = self._create_loader_strategy()
        self.sampler = self._create_sampler()

    def _create_loader_strategy(self):
        strategies = {
            'camels': CAMELSLoaderStrategy(),
            'csv': CSVLoaderStrategy(),
            'impro': IMPROLoaderStrategy(),
        }
        return strategies[self.format]
```

### 3. Hybrid Trainer

**è®¾è®¡æ¨¡å¼**: Strategy Pattern + Command Pattern

```python
# HBA-Model/src/dmg/trainers/hybrid_trainer.py

class HybridTrainer(BaseTrainer):
    """
    æ··åˆè®­ç»ƒå™¨ï¼Œæ”¯æŒï¼š
    1. Traditional Calibration (Spotpy): ç”¨äºçº¯ç‰©ç†æ¨¡å‹
    2. Gradient Descent (PyTorch): ç”¨äº dPL å’Œç¥ç»ç½‘ç»œ
    3. Hybrid Mode: å…ˆ Spotpy é¢„è®­ç»ƒï¼Œå† Gradient Fine-tune
    """

    def __init__(self, config, model, train_dataset, eval_dataset):
        super().__init__(config, model, train_dataset, eval_dataset)
        self.training_strategy = self._select_training_strategy()

    def _select_training_strategy(self):
        model_type = self.config['model']['type']

        if model_type == 'physics':
            return SpotpyCalibrationStrategy(self.config)
        elif model_type == 'dpl':
            return GradientDescentStrategy(self.config)
        elif model_type == 'hybrid':
            return HybridStrategy(self.config)  # Spotpy â†’ Gradient
        else:
            return NeuralNetworkStrategy(self.config)
```

### 4. Experiment Task System

**è®¾è®¡æ¨¡å¼**: Command Pattern + Registry Pattern

```python
# HBA-Model/src/dmg/experiments/base_experiment.py

class BaseExperiment(ABC):
    """
    å®éªŒæŠ½è±¡åŸºç±»

    æ‰€æœ‰å®éªŒéµå¾ªç»Ÿä¸€æµç¨‹ï¼š
    1. Setup: æ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–
    2. Execute: è¿è¡Œå®éªŒé€»è¾‘
    3. Evaluate: è®¡ç®—æŒ‡æ ‡
    4. Report: ä¿å­˜ç»“æœå’Œå¯è§†åŒ–
    """

    def __init__(self, config: DictConfig):
        self.config = config
        self.results = {}

    @abstractmethod
    def setup(self) -> None:
        """å‡†å¤‡å®éªŒç¯å¢ƒ"""

    @abstractmethod
    def execute(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®éªŒé€»è¾‘"""

    @abstractmethod
    def evaluate(self) -> Dict[str, float]:
        """è¯„ä¼°å®éªŒç»“æœ"""

    @abstractmethod
    def report(self, output_dir: Path) -> None:
        """ç”ŸæˆæŠ¥å‘Š"""

    def run(self) -> Dict[str, Any]:
        """å®Œæ•´å®éªŒæµç¨‹"""
        self.setup()
        results = self.execute()
        metrics = self.evaluate()
        self.report(Path(self.config['output_dir']))
        return {'results': results, 'metrics': metrics}
```

```python
# HBA-Model/src/dmg/experiments/learning_curves.py

class LearningCurveExperiment(BaseExperiment):
    """
    å®éªŒ1ï¼šå­¦ä¹ æ›²çº¿åˆ†æ

    ç›®æ ‡ï¼šè¯„ä¼°ä¸åŒæ¨¡å‹åœ¨ä¸åŒè®­ç»ƒæ ·æœ¬é‡ä¸‹çš„å­¦ä¹ èƒ½åŠ›

    é…ç½®ç¤ºä¾‹ï¼š
    experiment:
      name: learning_curves
      sample_sizes: [50, 100, 500, 1000, 2000, 3654]
      n_replicates: 30
      models: ['HBV', 'GR4J', 'LSTM', 'dPL-HBV']
      metrics: ['KGE', 'H_conditional', 'H_normalized']
    """

    def execute(self) -> Dict[str, Any]:
        results = {}

        for model_name in self.config['experiment']['models']:
            model_results = {}

            for sample_size in self.config['experiment']['sample_sizes']:
                # ç”Ÿæˆé‡‡æ ·ç´¢å¼•
                sampling_indices = self.sampler.generate_samples(
                    n_total=len(self.train_data),
                    sample_size=sample_size,
                    n_replicates=self.config['experiment']['n_replicates'],
                    strategy='consecutive_random'
                )

                replicate_results = []

                for rep_idx, indices in enumerate(sampling_indices):
                    # è®­ç»ƒæ¨¡å‹
                    trained_model = self.train_model(
                        model_name,
                        self.train_data[indices]
                    )

                    # è¯„ä¼°æ¨¡å‹
                    predictions = trained_model.predict(self.test_data)
                    metrics = self.compute_metrics(predictions)
                    replicate_results.append(metrics)

                model_results[sample_size] = replicate_results

            results[model_name] = model_results

        return results
```

---

## ğŸ“ é…ç½®æ–‡ä»¶è®¾è®¡

### å®éªŒé…ç½®ç»“æ„

```yaml
# HBA-Model/conf/experiments/learning_curves.yaml

# å®éªŒå…ƒæ•°æ®
experiment:
  name: learning_curves
  description: "Analyze model learning ability vs training data size"
  type: replicated_sampling  # single, replicated_sampling, spatial_cv

# æ•°æ®é…ç½®
data:
  source: csv  # camels, csv, impro
  catchments: ['Iller', 'Saale', 'Selke']
  data_dir: ${oc.env:DATA_DIR,'./Dataset/IMPRO_catchment_data_infotheo'}

  # æ—¶é—´åˆ’åˆ†
  periods:
    train:
      start: '2001-01-01'
      end: '2010-12-31'
    test:
      start: '2012-01-01'
      end: '2015-12-31'
    warmup_days: 365

  # é‡‡æ ·ç­–ç•¥
  sampling:
    strategy: consecutive_random  # consecutive_random, douglas_peucker, stratified
    sample_sizes: [2, 10, 50, 100, 250, 500, 1000, 2000, 3000, 3654]
    n_replicates: 30
    seed: 42

# æ¨¡å‹é…ç½®
models:
  # çº¯ç‰©ç†æ¨¡å‹ï¼ˆä½¿ç”¨ Spotpy æ ¡å‡†ï¼‰
  - name: HBV
    type: physics
    training:
      method: spotpy
      algorithm: lhs
      n_iterations: 500
      objective: kge
    parameters:
      TT: [0.0, -2.5, 2.5]  # [default, min, max]
      CFMAX: [3.5, 1.0, 10.0]
      FC: [250.0, 50.0, 500.0]
      BETA: [2.0, 1.0, 6.0]
      # ... å…¶ä»–å‚æ•°

  - name: GR4J
    type: physics
    training:
      method: spotpy
      algorithm: lhs
      n_iterations: 500
      objective: kge

  # æ•°æ®é©±åŠ¨æ¨¡å‹ï¼ˆæ¢¯åº¦ä¸‹é™è®­ç»ƒï¼‰
  - name: LSTM
    type: neural_network
    training:
      method: gradient_descent
      optimizer: Adam
      learning_rate: 0.001
      epochs: 50
      batch_size: 256
    architecture:
      sequence_length: 365
      hidden_size: 64
      num_layers: 2
      dropout: 0.1

  # å¾®åˆ†å‚æ•°å­¦ä¹ æ¨¡å‹ï¼ˆæ··åˆè®­ç»ƒï¼‰
  - name: dPL-HBV
    type: dpl
    training:
      method: hybrid  # spotpy_pretrain + gradient_finetune
      pretrain:
        algorithm: lhs
        n_iterations: 200
      finetune:
        optimizer: Adadelta
        learning_rate: 1.0
        epochs: 30
    components:
      nn_model:
        type: LSTM
        hidden_size: 32
        num_layers: 1
      phy_model:
        type: HBV
        learnable_params: ['FC', 'BETA', 'K0', 'K1', 'K2']  # LSTM å­¦ä¹ è¿™äº›å‚æ•°

# è¯„ä¼°æŒ‡æ ‡
metrics:
  performance:
    - name: KGE
      description: Kling-Gupta Efficiency
    - name: NSE
      description: Nash-Sutcliffe Efficiency
    - name: RMSE
      description: Root Mean Square Error

  information_theory:
    - name: H_conditional
      description: Conditional Entropy
      params:
        n_bins: 12
    - name: H_normalized
      description: Normalized Entropy
      params:
        n_bins: 12
    - name: mutual_information
      description: Mutual Information between obs and sim

# è¾“å‡ºé…ç½®
output:
  base_dir: ./results/experiments/learning_curves
  save_format: ['pickle', 'csv', 'netcdf']

  visualization:
    enabled: true
    plots:
      - type: learning_curve
        x_axis: sample_size
        y_axis: H_conditional
        groupby: model
        style: median_with_iqr  # median + 25th-75th percentile
      - type: metric_comparison
        metrics: ['KGE', 'H_conditional']
        models: all

  reports:
    generate_latex: false
    generate_html: true

# è®¡ç®—èµ„æº
compute:
  device: cuda  # cuda, cpu
  num_workers: 4
  parallel_replicates: true  # å¹¶è¡Œè¿è¡Œ replicates

# éšæœºç§å­ï¼ˆå¯é‡å¤æ€§ï¼‰
random_seed: 42
```

### é‡‡æ ·ç­–ç•¥é…ç½®

```yaml
# HBA-Model/conf/sampling/douglas_peucker.yaml

name: douglas_peucker
description: "Douglas-Peucker algorithm for information-driven sampling"

algorithm:
  type: iterative_reduction
  distance_metric: perpendicular_distance

  # è·ç¦»è®¡ç®—æ–¹å¼
  feature_space:
    - discharge  # ä½¿ç”¨å¾„æµä½œä¸ºç‰¹å¾
    - precip     # å¯é€‰ï¼šå¤šç»´ç‰¹å¾ç©ºé—´

  normalization: minmax  # minmax, zscore, none

  # è¿­ä»£å‚æ•°
  initial_sample: full_timeseries
  reduction_strategy: greedy  # greedy, balanced

# ç”¨äºå®éªŒ2
experiment:
  target_sample_sizes: [50, 100, 250, 500, 1000]
  comparison_baseline: random_sampling
```

---

## ğŸ”„ è¿ç§»è·¯å¾„

### Phase 1: åŸºç¡€è®¾æ–½ (Week 1-2)

1. âœ… **Model Adapter**
   - å®ç° `LegacyModelAdapter` åŸºç±»
   - ç§»æ¤ HBV æ¨¡å‹åˆ° PyTorch (`hbv_torch.py`)
   - ç§»æ¤ GR4J æ¨¡å‹åˆ° PyTorch (`gr4j_torch.py`)
   - å•å…ƒæµ‹è¯•ï¼šéªŒè¯æ•°å€¼ä¸€è‡´æ€§

2. âœ… **Data Loader Extension**
   - æ‰©å±• `HydroLoader` æ”¯æŒ CSV/ASCII
   - å®ç° `LegacyCSVLoader`
   - é›†æˆé‡‡æ ·ç­–ç•¥ (Douglas-Peucker, Random, Stratified)
   - å•å…ƒæµ‹è¯•ï¼šæ•°æ®åŠ è½½å’Œé‡‡æ ·

3. âœ… **Hybrid Trainer**
   - å®ç° Spotpy æ ¡å‡†ç­–ç•¥
   - é›†æˆåˆ° `HybridTrainer`
   - å•å…ƒæµ‹è¯•ï¼šæ ¡å‡†æ”¶æ•›æ€§

### Phase 2: å®éªŒç³»ç»Ÿ (Week 3-4)

4. âœ… **Experiment Framework**
   - å®ç° `BaseExperiment` æŠ½è±¡ç±»
   - åˆ›å»ºå®éªŒæ³¨å†Œè¡¨ (`TaskRegistry`)
   - å®ç°å®éªŒæ‰§è¡Œå™¨ (`TaskExecutor`)

5. âœ… **Migrate Experiments**
   - å®éªŒ1: `LearningCurveExperiment`
   - å®éªŒ2: `SamplingStrategyExperiment`
   - å®éªŒ3: `InformationContentExperiment`
   - å®éªŒ4: `SpatialDistributionExperiment`

6. âœ… **Metrics Integration**
   - æ‰©å±• `dmg/core/calc/metrics.py` æ·»åŠ ç†µæŒ‡æ ‡
   - åˆ›å»º `EntropyMetrics` Pydantic æ¨¡å‹
   - é›†æˆåˆ°è¯„ä¼°æµç¨‹

### Phase 3: é…ç½®ä¸å…¥å£ (Week 5)

7. âœ… **Configuration System**
   - è®¾è®¡å®Œæ•´çš„ YAML é…ç½®ç»“æ„
   - åˆ›å»ºé…ç½®éªŒè¯ Schema
   - å®ç°é…ç½®ç»§æ‰¿å’Œç»„åˆ

8. âœ… **Unified Entry Point**
   - é‡æ„ `dmg/__main__.py`
   - æ·»åŠ å®éªŒæ¨¡å¼æ”¯æŒ
   - CLI å‚æ•°è§£æ

9. âœ… **Documentation**
   - API æ–‡æ¡£
   - ç”¨æˆ·æ‰‹å†Œ
   - ç¤ºä¾‹å®éªŒé…ç½®

### Phase 4: éªŒè¯ä¸ä¼˜åŒ– (Week 6)

10. âœ… **Integration Testing**
    - ç«¯åˆ°ç«¯æµ‹è¯•æ‰€æœ‰å®éªŒ
    - æ€§èƒ½åŸºå‡†æµ‹è¯•
    - å¯¹æ¯” Legacy ç»“æœéªŒè¯æ­£ç¡®æ€§

11. âœ… **Optimization**
    - GPU åŠ é€Ÿä¼˜åŒ–
    - å¹¶è¡ŒåŒ–å®éªŒå¤åˆ¶
    - å†…å­˜ä½¿ç”¨ä¼˜åŒ–

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### å‘½ä»¤è¡Œæ¥å£

```bash
# è¿è¡Œå•ä¸ªå®éªŒï¼ˆä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
python -m dmg --config-name=learning_curves

# è¿è¡Œå®éªŒå¹¶è¦†ç›–å‚æ•°
python -m dmg --config-name=learning_curves \
  experiment.sample_sizes=[50,500,1000] \
  data.catchments=['Iller'] \
  compute.device=cuda

# è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆæ‰¹å¤„ç†ï¼‰
python -m dmg --config-name=run_all_experiments

# è¿è¡Œç‰¹å®šå®éªŒå­é›†
python -m dmg --config-name=run_all_experiments \
  experiments=[learning_curves,sampling_strategies]

# Quick test mode (å¼€å‘è°ƒè¯•)
python -m dmg --config-name=learning_curves \
  experiment.n_replicates=3 \
  experiment.sample_sizes=[50,500] \
  data.catchments=['Iller']
```

### Python API

```python
from omegaconf import OmegaConf
from dmg.experiments import ExperimentRegistry

# åŠ è½½é…ç½®
config = OmegaConf.load('conf/experiments/learning_curves.yaml')

# åˆ›å»ºå®éªŒ
experiment = ExperimentRegistry.create('learning_curves', config)

# è¿è¡Œå®éªŒ
results = experiment.run()

# è®¿é—®ç»“æœ
print(results['metrics']['HBV']['KGE']['median'])
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¹¶è¡ŒåŒ–

```python
# å¹¶è¡Œè¿è¡Œ replicates
from concurrent.futures import ProcessPoolExecutor

def train_single_replicate(args):
    model, data, indices = args
    trained_model = train_model(model, data[indices])
    return evaluate(trained_model, test_data)

with ProcessPoolExecutor(max_workers=8) as executor:
    results = list(executor.map(train_single_replicate, replicate_args))
```

### 2. GPU æ‰¹å¤„ç†

```python
# æ‰¹é‡å‰å‘ä¼ æ’­ï¼ˆé™ä½ GPU kernel launch overheadï¼‰
batch_predictions = []
for batch_indices in batched(all_indices, batch_size=32):
    batch_data = stack_data([data[i] for i in batch_indices])
    predictions = model(batch_data)  # å•æ¬¡ GPU è°ƒç”¨
    batch_predictions.extend(predictions)
```

### 3. ç¼“å­˜æœºåˆ¶

```python
# ç¼“å­˜æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ç»“æœ
@functools.lru_cache(maxsize=128)
def load_and_preprocess_catchment(catchment_name, data_dir):
    data = load_catchment(catchment_name, data_dir)
    return preprocess(data)
```

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# tests/test_model_adapter.py
def test_hbv_torch_numerical_consistency():
    """éªŒè¯ PyTorch HBV ä¸ Legacy NumPy HBV æ•°å€¼ä¸€è‡´æ€§"""
    legacy_model = LegacyHBV()
    torch_model = HBVTorch()

    # ä½¿ç”¨ç›¸åŒå‚æ•°å’Œè¾“å…¥
    params = {...}
    forcings = {...}

    legacy_output = legacy_model.simulate(forcings, params)
    torch_output = torch_model(forcings, params).detach().numpy()

    np.testing.assert_allclose(legacy_output, torch_output, rtol=1e-5)
```

### é›†æˆæµ‹è¯•

```python
# tests/test_experiments.py
def test_learning_curve_experiment_end_to_end():
    """ç«¯åˆ°ç«¯æµ‹è¯•å­¦ä¹ æ›²çº¿å®éªŒ"""
    config = load_test_config('learning_curves_quick.yaml')
    experiment = LearningCurveExperiment(config)
    results = experiment.run()

    # éªŒè¯ç»“æœç»“æ„
    assert 'HBV' in results['results']
    assert 50 in results['results']['HBV']
    assert len(results['results']['HBV'][50]) == config.experiment.n_replicates
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Staudinger, M., et al. (2025). "Learning curves and sampling strategies for hydrological models"
2. Kratzert, F., et al. (2019). "Towards learning universal, regional, and local hydrological behaviors via machine learning applied to large-sample datasets"
3. Seibert, J., & Vis, M. J. P. (2012). "Teaching hydrological modeling with a user-friendly catchment-runoff-model software package"

---

## ğŸ”— ç›¸å…³æ–‡ä»¶ç´¢å¼•

### æ ¸å¿ƒå®ç°æ–‡ä»¶

- æ¨¡å‹é€‚é…: `HBA-Model/src/dmg/models/phy_models/hbv_torch.py`
- æ•°æ®åŠ è½½: `HBA-Model/src/dmg/core/data/loaders/legacy_csv_loader.py`
- æ··åˆè®­ç»ƒ: `HBA-Model/src/dmg/trainers/hybrid_trainer.py`
- å®éªŒåŸºç±»: `HBA-Model/src/dmg/experiments/base_experiment.py`
- å­¦ä¹ æ›²çº¿: `HBA-Model/src/dmg/experiments/learning_curves.py`

### é…ç½®æ–‡ä»¶

- å®éªŒé…ç½®: `HBA-Model/conf/experiments/`
- é‡‡æ ·ç­–ç•¥: `HBA-Model/conf/sampling/`
- æ¨¡å‹é…ç½®: `HBA-Model/conf/models/`

### æµ‹è¯•æ–‡ä»¶

- å•å…ƒæµ‹è¯•: `HBA-Model/tests/unit/`
- é›†æˆæµ‹è¯•: `HBA-Model/tests/integration/`
- ç«¯åˆ°ç«¯æµ‹è¯•: `HBA-Model/tests/e2e/`

---

**Last Updated**: 2025-11-25
**Version**: 1.0
**Author**: Senior Python Architect & Computational Hydrology Expert
